// <auto-generated/>
#pragma warning disable CS0618
using Microsoft.Kiota.Abstractions.Extensions;
using Microsoft.Kiota.Abstractions.Serialization;
using System.Collections.Generic;
using System.IO;
using System;
namespace Soenneker.Cloudflare.OpenApiClient.Models
{
    [global::System.CodeDom.Compiler.GeneratedCode("Kiota", "1.0.0")]
    #pragma warning disable CS1591
    public partial class Workers_ai_post_run_cf_deepgram_nova_3 : IAdditionalDataHolder, IParsable
    #pragma warning restore CS1591
    {
        /// <summary>Stores additional data not described in the OpenAPI description found when deserializing. Can be used for serialization as well.</summary>
        public IDictionary<string, object> AdditionalData { get; set; }
        /// <summary>The audio property</summary>
#if NETSTANDARD2_1_OR_GREATER || NETCOREAPP3_1_OR_GREATER
#nullable enable
        public global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_audio? Audio { get; set; }
#nullable restore
#else
        public global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_audio Audio { get; set; }
#endif
        /// <summary>The number of channels in the submitted audio</summary>
        public double? Channels { get; set; }
        /// <summary>Custom intents you want the model to detect within your input audio if present</summary>
#if NETSTANDARD2_1_OR_GREATER || NETCOREAPP3_1_OR_GREATER
#nullable enable
        public string? CustomIntent { get; set; }
#nullable restore
#else
        public string CustomIntent { get; set; }
#endif
        /// <summary>Sets how the model will interpret intents submitted to the custom_intent param. When strict, the model will only return intents submitted using the custom_intent param. When extended, the model will return its own detected intents in addition those submitted using the custom_intents param</summary>
        public global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_custom_intent_mode? CustomIntentMode { get; set; }
        /// <summary>Custom topics you want the model to detect within your input audio or text if present Submit up to 100</summary>
#if NETSTANDARD2_1_OR_GREATER || NETCOREAPP3_1_OR_GREATER
#nullable enable
        public string? CustomTopic { get; set; }
#nullable restore
#else
        public string CustomTopic { get; set; }
#endif
        /// <summary>Sets how the model will interpret strings submitted to the custom_topic param. When strict, the model will only return topics submitted using the custom_topic param. When extended, the model will return its own detected topics in addition to those submitted using the custom_topic param.</summary>
        public global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_custom_topic_mode? CustomTopicMode { get; set; }
        /// <summary>Identifies and extracts key entities from content in submitted audio</summary>
        public bool? DetectEntities { get; set; }
        /// <summary>Identifies the dominant language spoken in submitted audio</summary>
        public bool? DetectLanguage { get; set; }
        /// <summary>Recognize speaker changes. Each word in the transcript will be assigned a speaker number starting at 0</summary>
        public bool? Diarize { get; set; }
        /// <summary>Identify and extract key entities from content in submitted audio</summary>
        public bool? Dictation { get; set; }
        /// <summary>Specify the expected encoding of your submitted audio</summary>
        public global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_encoding? Encoding { get; set; }
        /// <summary>Indicates how long model will wait to detect whether a speaker has finished speaking or pauses for a significant period of time. When set to a value, the streaming endpoint immediately finalizes the transcription for the processed time range and returns the transcript with a speech_final parameter set to true. Can also be set to false to disable endpointing</summary>
#if NETSTANDARD2_1_OR_GREATER || NETCOREAPP3_1_OR_GREATER
#nullable enable
        public string? Endpointing { get; set; }
#nullable restore
#else
        public string Endpointing { get; set; }
#endif
        /// <summary>Arbitrary key-value pairs that are attached to the API response for usage in downstream processing</summary>
#if NETSTANDARD2_1_OR_GREATER || NETCOREAPP3_1_OR_GREATER
#nullable enable
        public string? Extra { get; set; }
#nullable restore
#else
        public string Extra { get; set; }
#endif
        /// <summary>Filler Words can help transcribe interruptions in your audio, like &apos;uh&apos; and &apos;um&apos;</summary>
        public bool? FillerWords { get; set; }
        /// <summary>Specifies whether the streaming endpoint should provide ongoing transcription updates as more audio is received. When set to true, the endpoint sends continuous updates, meaning transcription results may evolve over time. Note: Supported only for webosockets.</summary>
        public bool? InterimResults { get; set; }
        /// <summary>Key term prompting can boost or suppress specialized terminology and brands.</summary>
#if NETSTANDARD2_1_OR_GREATER || NETCOREAPP3_1_OR_GREATER
#nullable enable
        public string? Keyterm { get; set; }
#nullable restore
#else
        public string Keyterm { get; set; }
#endif
        /// <summary>Keywords can boost or suppress specialized terminology and brands.</summary>
#if NETSTANDARD2_1_OR_GREATER || NETCOREAPP3_1_OR_GREATER
#nullable enable
        public string? Keywords { get; set; }
#nullable restore
#else
        public string Keywords { get; set; }
#endif
        /// <summary>The BCP-47 language tag that hints at the primary spoken language. Depending on the Model and API endpoint you choose only certain languages are available.</summary>
#if NETSTANDARD2_1_OR_GREATER || NETCOREAPP3_1_OR_GREATER
#nullable enable
        public string? Language { get; set; }
#nullable restore
#else
        public string Language { get; set; }
#endif
        /// <summary>Spoken measurements will be converted to their corresponding abbreviations.</summary>
        public bool? Measurements { get; set; }
        /// <summary>Opts out requests from the Deepgram Model Improvement Program. Refer to our Docs for pricing impacts before setting this to true. https://dpgr.am/deepgram-mip.</summary>
        public bool? MipOptOut { get; set; }
        /// <summary>Mode of operation for the model representing broad area of topic that will be talked about in the supplied audio</summary>
        public global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_mode? Mode { get; set; }
        /// <summary>Transcribe each audio channel independently.</summary>
        public bool? Multichannel { get; set; }
        /// <summary>Numerals converts numbers from written format to numerical format.</summary>
        public bool? Numerals { get; set; }
        /// <summary>Splits audio into paragraphs to improve transcript readability.</summary>
        public bool? Paragraphs { get; set; }
        /// <summary>Profanity Filter looks for recognized profanity and converts it to the nearest recognized non-profane word or removes it from the transcript completely.</summary>
        public bool? ProfanityFilter { get; set; }
        /// <summary>Add punctuation and capitalization to the transcript.</summary>
        public bool? Punctuate { get; set; }
        /// <summary>Redaction removes sensitive information from your transcripts.</summary>
#if NETSTANDARD2_1_OR_GREATER || NETCOREAPP3_1_OR_GREATER
#nullable enable
        public string? Redact { get; set; }
#nullable restore
#else
        public string Redact { get; set; }
#endif
        /// <summary>Search for terms or phrases in submitted audio and replaces them.</summary>
#if NETSTANDARD2_1_OR_GREATER || NETCOREAPP3_1_OR_GREATER
#nullable enable
        public string? Replace { get; set; }
#nullable restore
#else
        public string Replace { get; set; }
#endif
        /// <summary>Search for terms or phrases in submitted audio.</summary>
#if NETSTANDARD2_1_OR_GREATER || NETCOREAPP3_1_OR_GREATER
#nullable enable
        public string? Search { get; set; }
#nullable restore
#else
        public string Search { get; set; }
#endif
        /// <summary>Recognizes the sentiment throughout a transcript or text.</summary>
        public bool? Sentiment { get; set; }
        /// <summary>Apply formatting to transcript output. When set to true, additional formatting will be applied to transcripts to improve readability.</summary>
        public bool? SmartFormat { get; set; }
        /// <summary>Detect topics throughout a transcript or text.</summary>
        public bool? Topics { get; set; }
        /// <summary>Indicates how long model will wait to send an UtteranceEnd message after a word has been transcribed. Use with interim_results. Note: Supported only for webosockets.</summary>
        public bool? UtteranceEndMs { get; set; }
        /// <summary>Segments speech into meaningful semantic units.</summary>
        public bool? Utterances { get; set; }
        /// <summary>Seconds to wait before detecting a pause between words in submitted audio.</summary>
        public double? UttSplit { get; set; }
        /// <summary>Indicates that speech has started. You&apos;ll begin receiving Speech Started messages upon speech starting. Note: Supported only for webosockets.</summary>
        public bool? VadEvents { get; set; }
        /// <summary>
        /// Instantiates a new <see cref="global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3"/> and sets the default values.
        /// </summary>
        public Workers_ai_post_run_cf_deepgram_nova_3()
        {
            AdditionalData = new Dictionary<string, object>();
        }
        /// <summary>
        /// Creates a new instance of the appropriate class based on discriminator value
        /// </summary>
        /// <returns>A <see cref="global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3"/></returns>
        /// <param name="parseNode">The parse node to use to read the discriminator value and create the object</param>
        public static global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3 CreateFromDiscriminatorValue(IParseNode parseNode)
        {
            _ = parseNode ?? throw new ArgumentNullException(nameof(parseNode));
            return new global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3();
        }
        /// <summary>
        /// The deserialization information for the current model
        /// </summary>
        /// <returns>A IDictionary&lt;string, Action&lt;IParseNode&gt;&gt;</returns>
        public virtual IDictionary<string, Action<IParseNode>> GetFieldDeserializers()
        {
            return new Dictionary<string, Action<IParseNode>>
            {
                { "audio", n => { Audio = n.GetObjectValue<global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_audio>(global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_audio.CreateFromDiscriminatorValue); } },
                { "channels", n => { Channels = n.GetDoubleValue(); } },
                { "custom_intent", n => { CustomIntent = n.GetStringValue(); } },
                { "custom_intent_mode", n => { CustomIntentMode = n.GetEnumValue<global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_custom_intent_mode>(); } },
                { "custom_topic", n => { CustomTopic = n.GetStringValue(); } },
                { "custom_topic_mode", n => { CustomTopicMode = n.GetEnumValue<global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_custom_topic_mode>(); } },
                { "detect_entities", n => { DetectEntities = n.GetBoolValue(); } },
                { "detect_language", n => { DetectLanguage = n.GetBoolValue(); } },
                { "diarize", n => { Diarize = n.GetBoolValue(); } },
                { "dictation", n => { Dictation = n.GetBoolValue(); } },
                { "encoding", n => { Encoding = n.GetEnumValue<global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_encoding>(); } },
                { "endpointing", n => { Endpointing = n.GetStringValue(); } },
                { "extra", n => { Extra = n.GetStringValue(); } },
                { "filler_words", n => { FillerWords = n.GetBoolValue(); } },
                { "interim_results", n => { InterimResults = n.GetBoolValue(); } },
                { "keyterm", n => { Keyterm = n.GetStringValue(); } },
                { "keywords", n => { Keywords = n.GetStringValue(); } },
                { "language", n => { Language = n.GetStringValue(); } },
                { "measurements", n => { Measurements = n.GetBoolValue(); } },
                { "mip_opt_out", n => { MipOptOut = n.GetBoolValue(); } },
                { "mode", n => { Mode = n.GetEnumValue<global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_mode>(); } },
                { "multichannel", n => { Multichannel = n.GetBoolValue(); } },
                { "numerals", n => { Numerals = n.GetBoolValue(); } },
                { "paragraphs", n => { Paragraphs = n.GetBoolValue(); } },
                { "profanity_filter", n => { ProfanityFilter = n.GetBoolValue(); } },
                { "punctuate", n => { Punctuate = n.GetBoolValue(); } },
                { "redact", n => { Redact = n.GetStringValue(); } },
                { "replace", n => { Replace = n.GetStringValue(); } },
                { "search", n => { Search = n.GetStringValue(); } },
                { "sentiment", n => { Sentiment = n.GetBoolValue(); } },
                { "smart_format", n => { SmartFormat = n.GetBoolValue(); } },
                { "topics", n => { Topics = n.GetBoolValue(); } },
                { "utt_split", n => { UttSplit = n.GetDoubleValue(); } },
                { "utterance_end_ms", n => { UtteranceEndMs = n.GetBoolValue(); } },
                { "utterances", n => { Utterances = n.GetBoolValue(); } },
                { "vad_events", n => { VadEvents = n.GetBoolValue(); } },
            };
        }
        /// <summary>
        /// Serializes information the current object
        /// </summary>
        /// <param name="writer">Serialization writer to use to serialize this model</param>
        public virtual void Serialize(ISerializationWriter writer)
        {
            _ = writer ?? throw new ArgumentNullException(nameof(writer));
            writer.WriteObjectValue<global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_audio>("audio", Audio);
            writer.WriteDoubleValue("channels", Channels);
            writer.WriteStringValue("custom_intent", CustomIntent);
            writer.WriteEnumValue<global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_custom_intent_mode>("custom_intent_mode", CustomIntentMode);
            writer.WriteStringValue("custom_topic", CustomTopic);
            writer.WriteEnumValue<global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_custom_topic_mode>("custom_topic_mode", CustomTopicMode);
            writer.WriteBoolValue("detect_entities", DetectEntities);
            writer.WriteBoolValue("detect_language", DetectLanguage);
            writer.WriteBoolValue("diarize", Diarize);
            writer.WriteBoolValue("dictation", Dictation);
            writer.WriteEnumValue<global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_encoding>("encoding", Encoding);
            writer.WriteStringValue("endpointing", Endpointing);
            writer.WriteStringValue("extra", Extra);
            writer.WriteBoolValue("filler_words", FillerWords);
            writer.WriteBoolValue("interim_results", InterimResults);
            writer.WriteStringValue("keyterm", Keyterm);
            writer.WriteStringValue("keywords", Keywords);
            writer.WriteStringValue("language", Language);
            writer.WriteBoolValue("measurements", Measurements);
            writer.WriteBoolValue("mip_opt_out", MipOptOut);
            writer.WriteEnumValue<global::Soenneker.Cloudflare.OpenApiClient.Models.Workers_ai_post_run_cf_deepgram_nova_3_mode>("mode", Mode);
            writer.WriteBoolValue("multichannel", Multichannel);
            writer.WriteBoolValue("numerals", Numerals);
            writer.WriteBoolValue("paragraphs", Paragraphs);
            writer.WriteBoolValue("profanity_filter", ProfanityFilter);
            writer.WriteBoolValue("punctuate", Punctuate);
            writer.WriteStringValue("redact", Redact);
            writer.WriteStringValue("replace", Replace);
            writer.WriteStringValue("search", Search);
            writer.WriteBoolValue("sentiment", Sentiment);
            writer.WriteBoolValue("smart_format", SmartFormat);
            writer.WriteBoolValue("topics", Topics);
            writer.WriteBoolValue("utterance_end_ms", UtteranceEndMs);
            writer.WriteBoolValue("utterances", Utterances);
            writer.WriteDoubleValue("utt_split", UttSplit);
            writer.WriteBoolValue("vad_events", VadEvents);
            writer.WriteAdditionalData(AdditionalData);
        }
    }
}
#pragma warning restore CS0618
